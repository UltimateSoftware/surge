app {
  environment = "development"
  environment = ${?APP_ENVIRONMENT}
}

kafka {
  streams {
    # Clean up any Kafka Streams persistent state on startup, causing a full rebuild of any persisted state stores.
    # This is mostly useful if you manually change something in Kafka and Kafka Streams does not know how to pick up
    # where it left off - i.e. changing number of partitions, deleting and recreating a topic, manually changing
    # consumer offsets for this services consumer group, etc...
    wipe-state-on-start = false
    wipe-state-on-start = ${?KAFKA_STREAMS_CLEAN_ON_START}

    # Kafka streams commit interval. Kafka Streams will wait a max of this amount of time before committing results/flushing
    # KTables. In general, this is the delay an application will see before a state snapshot written to Kafka becomes available
    # in the aggregate state store KTable.  Increasing this gives better throughput but improves latency, decreasing gives lower
    # latency but worse throughput.
    commit-interval-ms = 3000
    commit-interval-ms = ${?KAFKA_STREAMS_COMMIT_INTERVAL_MS}

    # Number of standby replicas of state stores/Kafka Streams processors to keep. Additional standbys use
    # additional resources, but allow for better fault tolerance
    num-standby-replicas = 0
    num-standby-replicas = ${?KAFKA_STREAMS_NUM_STANDBY_REPLICAS}

    # Percent of on-heap memory to use for the Kafka Streams cache
    cache-heap-percentage = 0.25
    cache-heap-percentage = ${?KAFKA_STREAMS_CACHE_HEAP_PERCENT}

    state-dir = "/tmp/kafka-streams"
    state-dir = ${?KAFKA_STREAMS_STATE_DIR}

    # RocksDB is the underlying backing for the aggregate KTable state store.  You can tune performance of the
    # state store by tuning rocksdb.  The defaults for these are generally ok, and it is recommended to enable
    # dumping statistics to get a better view of the underlying performance before and while tuning settings here.
    rocks-db {
      # Number of concurrent compactions that should be run in parallel by rocksdb
      compaction-parallelism = 2
      compaction-parallelism = ${?KAFKA_STREAMS_ROCKSDB_COMPACTION_PARALLELISM}

      # The number of memtables that rocksdb will use when writing data to a KTable backed by rocksdb
      num-write-buffers = 2
      num-write-buffers = ${?KAFKA_STREAMS_ROCKSDB_NUM_WRITE_BUFFERS}

      # The size of each memtable that rocksdb will use when writing data to a KTable backed by rocksdb
      write-buffer-size-mb = 16
      write-buffer-size-mb = ${?KAFKA_STREAMS_ROCKSDB_WRITE_BUFFER_SIZE_MB}

      # The size of the block cache that rocksdb will use when writing data to a KTable backed by rocksdb
      block-cache-size-mb = 16
      block-cache-size-mb = ${?KAFKA_STREAMS_ROCKSDB_BLOCK_CACHE_SIZE_MB}

      # Used for debug and performance tuning, setting this to true will cause rocksdb to print
      # performance statistics to a log file in the same directory as rocksdb uses to spill sstfiles to disk
      dump-statistics = false
      dump-statistics = ${?KAFKA_STREAMS_ROCKSDB_DUMP_STATISTICS}

      # If dump-statistics is true, this is the interval at which statistics will be dumped by
      # rocksdb to the log file
      statistics-interval-seconds = 600
      statistics-interval-seconds = ${?KAFKA_STREAMS_ROCKSDB_STATISTICS_INTERVAL_SECONDS}
    }

    replay {
      reset-topic-timeout = 30 seconds
      reset-topic-timeout = ${?KAFKA_STREAMS_REPLAY_RESET_TOPIC_TIMEOUT}
      entire-process-timeout = 60 seconds
      entire-process-timeout = ${?KAFKA_STREAMS_REPLAY_TIMEOUT}
    }
  }

  brokers = "localhost:9092"
  brokers = ${?KAFKA_BROKERS}

  security.protocol = "PLAINTEXT"
  security.protocol = ${?KAFKA_SECURITY_PROTOCOL}

  sasl.mechanism = ""
  sasl.mechanism = ${?KAFKA_SASL_MECHANISM}

  sasl.jaas.conf = ""
  sasl.jaas.conf = ${?KAFKA_SASL_JAAS_CONF}

  sasl.kerberos.service.name = ""
  sasl.kerberos.service.name = ${?KAFKA_SASL_KERBEROS_SERVICE_NAME}

  ssl {
    keystore {
      location = ""
      location = ${?KAFKA_SSL_KEYSTORE_LOCATION}

      password = ""
      password = ${?KAFKA_SSL_KEYSTORE_PASSWORD}
    }

    truststore {
      location = ""
      location = ${?KAFKA_SSL_TRUSTSTORE_LOCATION}

      password = ""
      password = ${?KAFKA_SSL_TRUSTSTORE_PASSWORD}
    }

    key.password = ""
    key.password = ${?KAFKA_SSL_KEY_PASSWORD}
  }
}

surge {
  lifecycle-manager-actor {
    ask-timeout = 30 seconds
  }
  state-store-actor {
    # Amount of time to wait when sending a command to an aggregate before considering it to have timed out.
    ask-timeout = 30 seconds
    ask-timeout = ${?STATE_STORE_ACTOR_ASK_TIMEOUT}
    fetch-state-retry-interval = 2 seconds
    fetch-state-rety-interval = ${?STATE_FETCH_INTERVAL}
    initialize-state-retry-interval = 500 milliseconds
    initialize-state-retry-interval = ${?STATE_INIT_RETRY_INTERVAL}
    max-initialization-attempts = 10
    max-initialization-attempts = ${?STATE_MAX_INIT_ATTEMPTS}
    backoff {
      min-backoff = 3 seconds
      min-backoff = ${?STATE_STORE_ACTOR_BACKOFF_MIN}
      max-backoff = 30 seconds
      max-backoff = ${?STATE_STORE_ACTOR_BACKOFF_MAX}
      random-factor = 0.2
      random-factor = ${?STATE_STORE_ACTOR_BACKOFF_RANDOM}
      max-retries = 10
      max-retries = ${?STATE_STORE_ACTOR_BACKOFF_RETRIES}
    }
  }

  aggregate-actor {
    # Amount of time to keep an aggregate in memory if new commands for that aggregate are not being sent/processed.
    # It is recommended to keep this value higher than your setting for kafka.streams.commit-interval-ms, otherwise
    # quickly sending messages to an actor no longer in memory could incur a delay in actor initialization.
    idle-timeout = 30 seconds
    idle-timeout = ${?AGGREGATE_ACTOR_IDLE_TIMEOUT}

    # Amount of time to wait when sending a command to an aggregate before considering it to have timed out.
    ask-timeout = 30 seconds
    ask-timeout = ${?AGGREGATE_ACTOR_ASK_TIMEOUT}

    publish-failure-max-retries = 1
    publish-failure-max-retries = ${?SURGE_AGGREGATE_ACTOR_PUBLISH_FAILURE_MAX_RETRIES}
  }

  actor-registry {
    ask-timeout = 5 seconds
    ask-timeout = ${?SURGE_ACTOR_REGISTRY_ASK_TIMEOUT}

    resolve-actor-timeout = 10 seconds
    resolve-actor-timeout = ${?SURGE_ACTOR_REGISTRY_RESOLVE_ACTOR_TIMEOUT}
  }

  producer {
    ask-timeout = 20 seconds
    publish-timeout = 15 seconds
    publish-timeout = ${?SURGE_PRODUCER_PUBLISH_TIMEOUT}

    aggregate-state-current-timeout = 10 seconds
    aggregate-state-current-timeout = ${?SURGE_PRODUCER_AGGREGATE_STATE_CURRENT_TIMEOUT}

    enable-kafka-metrics = true
    enable-kafka-metrics = ${?SURGE_PRODUCER_ENABLE_KAFKA_METRICS}
  }

  kafka-streams {
    state-store-plugin = "rocksdb"

    enable-kafka-metrics = false # Some of these are tagged with the streams threadId or taskId which ends up being really noisy for Influx...
    enable-kafka-metrics = ${?SURGE_KAFKA_STREAMS_ENABLE_KAFKA_METRICS}
  }

  kafka-event-source {
    committer {
      max-batch = 1000
      max-batch = ${?SURGE_KAFKA_EVENT_SOURCE_COMMITTER_MAX_BATCH}

      max-interval = 10s
      max-interval = ${?SURGE_KAFKA_EVENT_SOURCE_COMMITTER_MAX_INTERVAL}

      parallelism = 100
      parallelism = ${?SURGE_KAFKA_EVENT_SOURCE_COMMITTER_PARALLELISM}
    }

    consumer {
      auto-offset-reset = "earliest"
      auto-offset-reset = ${?SURGE_KAFKA_EVENT_SOURCE_CONSUMER_AUTO_OFFSET_RESET}

      session-timeout = 10 seconds
      session-timeout = ${?SURGE_KAFKA_EVENT_SOURCE_CONSUMER_SESSION_TIMEOUT}

      partition-assignor = "range"
      partition-assignor = ${?SURGE_KAFKA_EVENT_SOURCE_CONSUMER_PARTITION_ASSIGNOR}
    }

    backoff {
      min = 1 second
      min = ${?SURGE_KAFKA_EVENT_SOURCE_BACKOFF_MIN}

      max = 10 seconds
      max = ${?SURGE_KAFKA_EVENT_SOURCE_BACKOFF_MAX}

      random-factor = 0.1
      random-factor = ${?SURGE_KAFKA_EVENT_SOURCE_BACKOFF_RANDOM_FACTOR}
    }

    kafka-metric-fetch-timeout = 15 seconds
    kafka-metric-fetch-timeout = ${?SURGE_KAFKA_EVENT_SOURCE_METRIC_FETCH_TIMEOUT}

    enable-kafka-metrics = true
    enable-kafka-metrics = ${?SURGE_KAFKA_EVENT_SOURCE_ENABLE_KAFKA_METRICS}
  }

  kafka-reuse-consumer-id = false
  kafka-reuse-consumer-id = ${?SURGE_KAFKA_REUSE_CONSUMER_GROUP_ID}

  health {
   signal-pattern-matcher-registry {}
   bus {
        signal-topic = "health.signal"
        signal-topic = ${?SURGE_HEALTH_BUS_SIGNAL_TOPIC}

        registration-topic = "health.registration"
        registration-topic = ${?SURGE_HEALTH_BUS_REGISTRATION_TOPIC}

        allowed-subscriber-count = 128
        allowed-subscriber-count = ${?SURGE_HEALTH_BUS_ALLOWED_SUBSCRIBER_COUNT}

        stream {
            start-on-init = true
            start-on-init = ${?SURGE_HEALTH_BUS_STREAM_START_ON_INIT}
        }
    }

    window {
        stream {
            delay = 5 seconds
            delay = ${?SURGE_HEALTH_WINDOW_STREAM_MAX_DELAY}
            max-size = 1000
            max-size = ${?SURGE_HEALTH_WINDOW_STREAM_MAX_SIZE}
            frequencies = [ 10 seconds ]
            frequencies = ${?SURGE_HEALTH_WINDOW_STREAM_FREQUENCIES}

            throttle {
                elements = 100
                duration = 5 seconds
            }
            advancer {
                # Slider implementation of Advancer
                type = "slider"
                # Wait until {buffer} signals have entered the window before attempting to slide the window {amount} signals
                buffer = 10
                buffer = ${?SURGE_HEALTH_WINDOW_STREAM_SLIDER_BUFFER}
                amount = 1
                amount = ${?SURGE_HEALTH_WINDOW_STREAM_SLIDER_AMOUNT}
            }
        }
    }

    window-actor {
        backoff {
            min-backoff = 5 seconds
            min-backoff = ${?SURGE_HEALTH_WINDOW_ACTOR_BACKOFF_MIN_BACKOFF}
            max-backoff = 20 seconds
            max-backoff = ${?SURGE_HEALTH_WINDOW_ACTOR_BACKOFF_MAX_BACKOFF}
            max-retries = 5
            max-retries = ${?SURGE_HEALTH_WINDOW_ACTOR_BACKOFF_MAX_RETRIES}
            random-factor = 0.2
            random-factor = ${?SURGE_HEALTH_WINDOW_ACTOR_BACKOFF_RANDOM_FACTOR}
        }
    }

    supervisor-actor {
        backoff {
            min-backoff = 5 seconds
            min-backoff = ${?SURGE_HEALTH_SIGNAL_SUPERVISOR_BACKOFF_MIN_BACKOFF}
            max-backoff = 20 seconds
            max-backoff = ${?SURGE_HEALTH_SIGNAL_SUPERVISOR_BACKOFF_MAX_BACKOFF}
            max-retries = 5
            max-retries = ${?SURGE_HEALTH_SIGNAL_SUPERVISOR_BACKOFF_MAX_RETRIES}
            random-factor = 0.2
            random-factor = ${?SURGE_HEALTH_SIGNAL_SUPERVISOR_BACKOFF_RANDOM_FACTOR}
        }
    }
  }
}

rocksdb {
  plugin-class = "surge.kafka.streams.RocksDBPersistencePlugin"
}

surge {
  dr-standby-enabled = false
  dr-standby-enabled = ${?SURGE_ENABLE_DR_STANDBY}
}

execution-context-prober {
    initial-delay = 3 seconds
    timeout = 50 milliseconds
    interval = 5 seconds
    num-probes = 7
    dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 1
      }
    }
}
